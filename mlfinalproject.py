# -*- coding: utf-8 -*-
"""MLFinalProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13s7XdHVB9jrGNYDjZ0PlPxI-ZxzbNtSs

**Project OverView**

This project is part of the BIT4333 – Introduction to Machine Learning final group assignment.
The objective is to design and implement a machine learning pipeline that solves a real-world decision-making problem.

---

**Pipeline Steps**

- Data Collection: Import dataset from open-source repository.

- Data Preprocessing: Handle missing values, clean data, and prepare features.

- Model Development: Train and compare at least two machine learning models.

- Model Evaluation: Use performance metrics to assess models.

- Deployment: Save models, present results, and upload to GitHub.


---

**Expected Outcomes**

- A complete Google Colab notebook with preprocessing, model training, evaluation, and testing.

- GitHub repository with all project files.

- Project report, presentation slides, and demo.

## First of all we have to import the Necessary Libraries and the Data set from Kaggale ##
"""

!pip install opendatasets

"""##We have to create a new Api token  in kaggle so we would be able to download the data set an work on it

##User name: ayatelsafi
  key: " d2d0c2ea997690e7e32de2ccb0998c5c"
"""

import opendatasets as od
od.download("https://www.kaggle.com/datasets/denkuznetz/food-delivery-time-prediction")

import pandas as pd
data = pd.read_csv("/content/food-delivery-time-prediction/Food_Delivery_Times.csv")

data.head()

"""##Inspect the dataset
this step is necessecary because:


*    It help us to understand  dataset structure  rows, columns, target, and features.

* Identify data types  numeric vs categorical (because it is needed for the needed for encoding).

* Detect missing values and  plan how to handle NaN or null data.

* Find duplicates clean them for better accuracy.

* View summary statistics so we would be able to spot outliers or unusual values.

* Guide preprocessing then decide what to drop, encode, or scale.

* Prevent errors later and ensures models don’t crash on bad input.


"""

print("Dataset shape:", data.shape)

# Column names
print("\nColumns in dataset:")
print(data.columns)

# First 5 rows
print("\nPreview of first 5 rows:")
print(data.head())

# Data types and missing values
print("\nDataset Info:")
print(data.info())

# Statistical summary for numerical columns
print("\nStatistical Summary:")
print(data.describe())

# Check missing values per column
print("\nMissing values per column:")
print(data.isnull().sum())

"""2. Clean the data (Data Preprocessing)




* Remove duplicates

* Handle missing values Replace NaN values with mean/median (for numbers) or mode (for categories).
* Fix column names (remove spaces, rename for clarity).





"""

# DATA CLEANING


import numpy as np

# 1. Remove duplicate rows
data.drop_duplicates(inplace=True)
print("Removed duplicates. New shape:", data.shape)

# 2. Handle missing values
for col in data.select_dtypes(include=[np.number]).columns:
    if data[col].isnull().sum() > 0:
        data[col].fillna(data[col].mean(), inplace=True)
        print(f"Filled missing values in numeric column '{col}' with mean.")

# Categorical columns
for col in data.select_dtypes(include=['object']).columns:
    if data[col].isnull().sum() > 0:
        data[col].fillna(data[col].mode()[0], inplace=True)
        print(f"Filled missing values in categorical column '{col}' with mode.")


# 3.  Clean column names: remove spaces and make lowercase
data.columns = data.columns.str.strip().str.replace(' ', '_').str.lower()

# 4. Final check for missing values
print("\nMissing values after cleaning:")
print(data.isnull().sum())

# 5. Preview cleaned dataset
print("\nPreview of cleaned dataset:")
print(data.head())

"""3. Encode categorical features

Models can’t handle text directly  so we need to convert strings into numbers:

*   Label Encoding (for categories like Weather: Sunny, Rainy).
*   One-hot Encoding (for categories with no order).






"""

# ENCODING CATEGORICAL DATA


from sklearn.preprocessing import LabelEncoder

# Automatically find all categorical (object/string) columns
categorical_cols = data.select_dtypes(include=['object']).columns
print("Categorical columns to encode:", categorical_cols.tolist())

# Apply Label Encoding to each categorical column
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])
    label_encoders[col] = le
    print(f"Encoded column: {col}")

# Preview encoded dataset
print("\nPreview after encoding:")
print(data.head())
print(data.columns)

"""## 4. Prepare Data for Modeling

Now that the data is cleaned and encoded, we need to prepare it for training machine learning models. This involves:

1.  Separating features (independent variables, X) from the target variable (dependent variable, y). The target variable is `delivery_time_min`.
2.  Splitting the data into training and testing sets. The training set will be used to train the models, and the testing set will be used to evaluate their performance on unseen data.
"""

from sklearn.model_selection import train_test_split

# Separate features (X) and target (y)
# We drop 'order_id' as it's an identifier and 'delivery_time_min' as it's the target.
X = data.drop(['order_id', 'delivery_time_min'], axis=1)
y = data['delivery_time_min']

# Split data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Original data shape:", data.shape)
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

"""#Secondly we have to develop our models

**Step 1: Split Dataset**
"""

from sklearn.model_selection import train_test_split

X = data.drop("delivery_time_min", axis=1)
y = data["delivery_time_min"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""**Step 2: Choose Two Models**
- Linear Regression – simple, interpretable baseline.

- Random Forest Regressor – more powerful, handles non-linear relationships well.

**Step 3: Train Models**
"""

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor

# Linear Regression
lr = LinearRegression()
lr.fit(X_train, y_train)

# Random Forest
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

"""**Step 4: Evaluate Models**
- Useing regression metrics: MAE, RMSE, and R².
"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import pandas as pd

models = {
    "Linear Regression": lr,
    "Random Forest": rf
}

rows = []
for name, model in models.items():
    # train predictions
    y_tr_pred = model.predict(X_train)
    # test predictions
    y_te_pred = model.predict(X_test)

    # metrics
    mae_tr  = mean_absolute_error(y_train, y_tr_pred)
    rmse_tr = np.sqrt(mean_squared_error(y_train, y_tr_pred))
    r2_tr   = r2_score(y_train, y_tr_pred)

    mae_te  = mean_absolute_error(y_test, y_te_pred)
    rmse_te = np.sqrt(mean_squared_error(y_test, y_te_pred))
    r2_te   = r2_score(y_test, y_te_pred)

    rows.append({
        "Model": name,
        "Train MAE": round(mae_tr, 2),
        "Train RMSE": round(rmse_tr, 2),
        "Train R²": round(r2_tr, 3),
        "Test MAE": round(mae_te, 2),
        "Test RMSE": round(rmse_te, 2),
        "Test R²": round(r2_te, 3),
        "Overfit? (R² gap)": round(r2_tr - r2_te, 3)
    })

eval_df = pd.DataFrame(rows).set_index("Model").sort_values("Test RMSE")
display(eval_df)

best_model_name = eval_df.index[0]
print(f"\nBest by Test RMSE: {best_model_name}")

"""Model Evaluation & Testing
We evaluated two models: Linear Regression and Random Forest, using MAE, RMSE, and R² on both training and testing sets.
The results showed that Linear Regression performed best on the test set with MAE = 7.29, RMSE = 10.45, and R² = 0.756.
Compared to Random Forest, it had lower error and more stable generalization.
The gap between training and testing scores was small, which means no strong overfitting.
Therefore, Linear Regression was selected as the best model for predicting delivery times.

##Now let's test our models efficacy "detecting overfitting"
"""

from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

for name, model in models.items():
    # Fit each model on training data
    model.fit(X_train, y_train)

    # Predictions
    y_train_pred = model.predict(X_train)
    y_test_pred  = model.predict(X_test)

    # Metrics
    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))
    rmse_test  = np.sqrt(mean_squared_error(y_test, y_test_pred))

    r2_train = r2_score(y_train, y_train_pred)
    r2_test  = r2_score(y_test, y_test_pred)

    print(f"{name}:")
    print(f"  Train RMSE = {rmse_train:.2f} | Test RMSE = {rmse_test:.2f}")
    print(f"  Train R²   = {r2_train:.3f} | Test R²   = {r2_test:.3f}")
    print("-"*50)

"""##Based on the results above: we choose the Linear Regression Model
- after comparing the R², the more close values the more we avoid overfitting

# Testing the Models with new Inputs

### 1. Linear Regression
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Load Dataset
# Assuming your dataset is already loaded as 'data'
X = data.drop(["delivery_time_min", "order_id"], axis=1)  # Drop target + irrelevant ID
y = data["delivery_time_min"]

# Split train & test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocessing
categorical_features = ["weather", "traffic_level", "time_of_day", "vehicle_type"]
numeric_features = ["distance_km", "preparation_time_min", "courier_experience_yrs"]

preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), numeric_features),
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features)
    ]
)

# Linear Regression Pipeline
lr_pipe = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("model", LinearRegression())
])

# Train
lr_pipe.fit(X_train, y_train)

# Predict on test
y_pred_lr = lr_pipe.predict(X_test)

# Model Evaluation
print("Linear Regression Performance")
print("RMSE:", round(np.sqrt(mean_squared_error(y_test, y_pred_lr)), 2))
print("MAE:", round(mean_absolute_error(y_test, y_pred_lr), 2))
print("R²:", round(r2_score(y_test, y_pred_lr), 3))

# Testing with New Input
new_case = pd.DataFrame([{
    "distance_km": 7.5,
    "weather": "Clear",
    "traffic_level": "High",
    "time_of_day": "Dinner",
    "vehicle_type": "Car",
    "preparation_time_min": 15,
    "courier_experience_yrs": 3
}])

predicted_time = lr_pipe.predict(new_case)
print("Predicted Delivery Time (min):", round(predicted_time[0], 2))

"""### 2. Linear Regression"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Load Dataset
# Assuming your dataset is already loaded as 'data'
X = data.drop(["delivery_time_min", "order_id"], axis=1)  # Drop target + irrelevant ID
y = data["delivery_time_min"]

# Split train & test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocessing
categorical_features = ["weather", "traffic_level", "time_of_day", "vehicle_type"]
numeric_features = ["distance_km", "preparation_time_min", "courier_experience_yrs"]

preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), numeric_features),
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features)
    ]
)

# Random Forest Pipeline
rf_pipe = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("model", RandomForestRegressor(random_state=42, n_estimators=100))
])

# Train
rf_pipe.fit(X_train, y_train)

# Predict on test
y_pred_rf = rf_pipe.predict(X_test)

# Model Evaluation
print("Random Forest Performance")
print("RMSE:", round(np.sqrt(mean_squared_error(y_test, y_pred_rf)), 2))
print("MAE:", round(mean_absolute_error(y_test, y_pred_rf), 2))
print("R²:", round(r2_score(y_test, y_pred_rf), 3))

# Testing with New Input
new_case = pd.DataFrame([{
    "distance_km": 7.5,
    "weather": "Clear",
    "traffic_level": "High",
    "time_of_day": "Dinner",
    "vehicle_type": "Car",
    "preparation_time_min": 15,
    "courier_experience_yrs": 3
}])

predicted_time_rf = rf_pipe.predict(new_case)
print("Random Forest Predicted Delivery Time (min):", round(predicted_time_rf[0], 2))

"""#Now it's time to visualise and conclude the results

**1. Model Performance Comparison (Bar Chart)**
- Compare RMSE & R² for Linear Regression vs Random Forest.
"""

import matplotlib.pyplot as plt
import numpy as np

# Collect results
results = {
    "Linear Regression": {"RMSE": 10.45, "R2": 0.756},
    "Random Forest": {"RMSE": 10.71, "R2": 0.744}
}

# Plot RMSE comparison
models = list(results.keys())
rmse_values = [results[m]["RMSE"] for m in models]

plt.figure(figsize=(6,4))
plt.bar(models, rmse_values, color=['skyblue','orange'])
plt.title("Model Comparison - RMSE (Lower is Better)")
plt.ylabel("RMSE")
plt.show()

# Plot R² comparison
r2_values = [results[m]["R2"] for m in models]

plt.figure(figsize=(6,4))
plt.bar(models, r2_values, color=['skyblue','orange'])
plt.title("Model Comparison - R² (Closer to 1 is Better)")
plt.ylabel("R²")
plt.show()

"""**2. Predicted vs Actual Delivery Times (Scatter Plot)**

- Proves your model predicts well: points close to the line mean good predictions.
"""

# Random Forest - Predicted vs Actual
y_pred = rf_pipe.predict(X_test)

plt.figure(figsize=(6,6))
plt.scatter(y_test, y_pred, alpha=0.6, color='teal')
plt.plot([y_test.min(), y_test.max()],
         [y_test.min(), y_test.max()],
         'r--', lw=2)  # perfect prediction line
plt.xlabel("Actual Delivery Time (min)")
plt.ylabel("Predicted Delivery Time (min)")
plt.title("Random Forest - Predicted vs Actual")
plt.show()


# Linear Regression - Predicted vs Actual
y_pred = lr_pipe.predict(X_test)

plt.figure(figsize=(6,6))
plt.scatter(y_test, y_pred, alpha=0.6, color='royalblue')
plt.plot([y_test.min(), y_test.max()],
         [y_test.min(), y_test.max()],
         'r--', lw=2)  # perfect prediction line
plt.xlabel("Actual Delivery Time (min)")
plt.ylabel("Predicted Delivery Time (min)")
plt.title("Linear Regression - Predicted vs Actual")
plt.show()

"""**3. Feature Importance (Explain Model Decisions)**
- Shows which features matter most for predictions (distance, traffic, etc.).
"""

import pandas as pd
import numpy as np


# Using Random Forest as best model
# Get feature names after preprocessing
encoded_features = rf_pipe.named_steps["preprocessor"].get_feature_names_out()
feature_names = list(encoded_features)

# Get feature importance from Random Forest inside pipeline
rf_model = rf_pipe.named_steps["model"]
feature_importance = rf_model.feature_importances_

# Create DataFrame
importance_df = pd.DataFrame({
    "Feature": feature_names,
    "Importance": feature_importance
}).sort_values(by="Importance", ascending=False)

# Plot
plt.figure(figsize=(8,5))
plt.barh(importance_df["Feature"], importance_df["Importance"], color="seagreen")
plt.xlabel("Importance Score")
plt.title("Random Forest - Feature Importance")
plt.gca().invert_yaxis()
plt.show()



# Using Linear Regression model
# Get feature names after preprocessing
encoded_features = lr_pipe.named_steps["preprocessor"].get_feature_names_out()
feature_names = list(encoded_features)

# Get coefficients from Linear Regression inside pipeline
lr_model = lr_pipe.named_steps["model"]
coefficients = lr_model.coef_

# Create DataFrame
importance_df = pd.DataFrame({
    "Feature": feature_names,
    "Coefficient": coefficients,
    "Absolute_Importance": np.abs(coefficients)
}).sort_values(by="Absolute_Importance", ascending=False)

# Plot
plt.figure(figsize=(8,5))
plt.barh(importance_df["Feature"], importance_df["Coefficient"], color="royalblue")
plt.xlabel("Coefficient Value")
plt.title("Linear Regression - Feature Coefficients")
plt.gca().invert_yaxis()
plt.show()